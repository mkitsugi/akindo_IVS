import os

import openai
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts.chat import (
    AIMessagePromptTemplate,
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
)
from llama_index import (
    Document,
    LangchainEmbedding,
    LLMPredictor,
    QuestionAnswerPrompt,
    ServiceContext,
    VectorStoreIndex,
)
from llama_index.prompts.chat_prompts import (
    CHAT_REFINE_PROMPT,
    CHAT_REFINE_PROMPT_TMPL_MSGS,
)
from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL
from llama_index.prompts.prompts import RefinePrompt

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
QA_PROMPT_TMPL = (
    "ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ \n"
    "---------------------\n"
    "{context_str}"
    "\n---------------------\n"
    "ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦ã€æ¬¡ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚: {query_str}\n"
)

CHAT_REFINE_PROMPT_TMPL_MSGS = [
    HumanMessagePromptTemplate.from_template("{query_str}"),
    AIMessagePromptTemplate.from_template("{existing_answer}"),
    HumanMessagePromptTemplate.from_template(
        """
    ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ \n"
    "---------------------\n"
    "{context_msg}"
    "\n---------------------\n"
    ã“ã®æƒ…å ±ãŒå›ç­”ã®æ”¹å–„ã«å½¹ç«‹ã¤ã‚ˆã†ãªã‚‰ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦å›ç­”ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚
    ã“ã®æƒ…å ±ãŒå›ç­”ã®æ”¹å–„ã«å½¹ç«‹ãŸãªã‘ã‚Œã°å…ƒã®å›ç­”ã‚’æ—¥æœ¬èªã§è¿”ã—ã¦ãã ã•ã„ã€‚
    """
    ),
]

CHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)

QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)
CHAT_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)


load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨åŒã˜ã ãŒä¸€å¿œæ˜ç¤ºã™ã‚‹
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
llm_predictor = LLMPredictor(llm)
llama_embed = LangchainEmbedding(
    embedding,
    embed_batch_size=1,
)
# ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
# ã“ã®ä¾‹ã§ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™
service_context = ServiceContext.from_defaults(
    llm_predictor=llm_predictor,
    embed_model=llama_embed,
)


# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
data = [
    "ãã†ãªã‚“ã ã‚ˆã­ã€æœ€è¿‘å›°ã£ã¦ã¦â€¦",
    "ä»•äº‹ãŒå¿™ã—ãã¦ã€å½¼å¥³ã‚‚è¦‹ã¤ã‘ã‚‰ã‚Œãªã„ã‚“ã ã‚ˆã­ãƒ¼ã€‚ã„ã„äººã„ãªã„ã‹ãªï¼Ÿå„ªã—ãã¦æ˜ã‚‹ã„äººï¼",
    "ãã†ãã†ã€è¶£å‘³ã¨ã‹ã¯ã‚´ãƒ«ãƒ•ã‚„ã£ã¦ã‚‹ã‚“ã ã€œâ™ª",
    "åƒ•ã®è¶£å‘³è¦šãˆã¦ã‚‹ãƒ¼ï¼Ÿ",
    "åƒ•ã®åå‰ã¯ãŸãã‚„ã§ã™ï¼",
    "ã‚ã‚Œã€ç›¸è«‡ä¹—ã£ã¦ãã‚Œãªã„ã®ï¼ŸğŸ¥º",
    "åƒ•ã®åå‰ã‚’è¦šãˆã¦ãã‚Œã‚‹ï¼Ÿæ‹“ä¹Ÿã£ã¦è¨€ã†ã‚“ã ã‘ã©",
    "åƒ•ã®åå‰ã‚’è¦šãˆã¦ã‚‹ã‹æ•™ãˆã¦ï¼Ÿ",
    "åƒ•ã®è¶£å‘³ã¯ï¼Ÿ",
    "åƒ•ã®è¶£å‘³è¦šãˆã¦ã‚‹ï¼Ÿ",
    "åƒ•ã®ã®å¥½ããªã‚‚ã®ã¯å”æšã’ãªã‚“ã ãƒ¼",
]
# List[Document]ã«å¤‰å½¢ã™ã‚‹
documents = [Document(text=item) for item in data]

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹
index = VectorStoreIndex.from_documents(documents, service_context=service_context)
index.storage_context.persist("azure/TEST/strage")

# ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã‚’ç”Ÿæˆã™ã‚‹
query_engine = index.as_query_engine(
    service_context=service_context, text_qa_template=QA_PROMPT, refine_template=CHAT_PROMPT
)

# ã‚¯ã‚¨ãƒªã‚’æŠ•ã’ã‚‹
qestion = "å¥½ããªé£Ÿã¹ç‰©ã¯ï¼Ÿ"
response = query_engine.query(qestion)
print(response)
# >>> å”æšã’ã§ã™ã€‚
print(response.get_formatted_sources(length=4096))
