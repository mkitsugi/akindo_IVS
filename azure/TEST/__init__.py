from asyncio import exceptions
import os
import openai
import json
import uuid
from azure.functions import HttpRequest, HttpResponse
from azure.cosmos import CosmosClient, exceptions
from langchain.memory import ConversationBufferMemory, ChatMessageHistory, ConversationEntityMemory
from langchain.schema import HumanMessage, AIMessage
from langchain.chains import ConversationChain
from langchain import LLMChain, PromptTemplate
from langchain.chat_models import ChatOpenAI
from dotenv import load_dotenv

# APIã‚­ãƒ¼ã®å–å¾—
load_dotenv()
OPENAI_KEY = os.environ.get('OPENAI_API_KEY')
openai.api_key = OPENAI_KEY

# Cosmos DBæ¥ç¶šè¨­å®š
COSMOS_DB_CONNECTION_STR = os.environ.get('COSMOS_DB_CONNECTION_STR')
DATABASE_NAME = "KG_AK"
client = CosmosClient.from_connection_string(COSMOS_DB_CONNECTION_STR)
database = client.get_database_client(DATABASE_NAME)
chat_container = database.get_container_client("Chats")
user_container = database.get_container_client("Users")
prompt_container = database.get_container_client("Prompts")
pref_container = database.get_container_client("Pref")

#Promptä¿ç®¡ç”¨ã®item_id(ä»®ç½®ã)
item_id = "a085dd38-9318-48cc-a8f4-49f0e4866190"

#llmãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
llm = ChatOpenAI(temperature=0.3,model="gpt-3.5-turbo-16k-0613")

#ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å–å¾—
def get_prompts():
    item_id = "a085dd38-9318-48cc-a8f4-49f0e4866190"
    item = prompt_container.read_item(item=item_id, partition_key=item_id)

    return item

#ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®å–å¾—
def get_user(user_id : str):
    user = user_container.read_item(item=user_id, partition_key=user_id)
    return user

#ãƒ¦ãƒ¼ã‚¶ãƒ¼å±æ€§ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
def upload_userpref(user_id:str, response:dict)-> None:
    try:
        if response != "":
            print("response:",response)
            response_dict = json.loads(response)
            response_dict["user_id"] = user_id
            response_dict["id"] = user_id

            print("response_dict:",response_dict)
            # Add data to container
            pref_container.upsert_item(response_dict)
            print("Data inserted successfully.")
    except exceptions.CosmosResourceNotFoundError:
        print("Error.")

def fetch_userpref(user_id:str):
    try:
        pref = pref_container.read_item(item=user_id, partition_key=user_id)
        return pref
    except exceptions.CosmosResourceNotFoundError:
        return {}
    
def update_userpref(current:dict,new:dict)-> None:

    existing_data = current
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ç¢ºèª
    merged_data = {**existing_data, **new}
    
    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›´æ–°
    if merged_data != existing_data:
        pref_container.upsert_item(merged_data)
    
    return merged_data

##ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
SUMMARY_QUESTION_TYPE_TEMPLATE="""
ã‚ãªãŸã¯ã‚ãªãŸãŒæŒã£ã¦ã„ã‚‹æƒ…å ±ã‚’é©åˆ‡ã«è¦ç´„ã—ã¦ç›¸æ‰‹ã«èª¬æ˜ã™ã‚‹ã®ãŒéå¸¸ã«ã†ã¾ã„ã¨æ€ã‚ã‚Œã¦ã„ã‚‹äººé–“ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å¿œç­”ã«å¯¾ã—ã¦é©åˆ‡ã«è¿”ã—ã¦ãã ã•ã„

#å¿œç­”: 

#å‡ºåŠ›: 

"""

base_prompt = """ã‚ãªãŸã¯ä»¥ä¸‹ã®è¨­å®šã‚’æŒã£ãŸchatbotã§ã™\
               # è¨­å®š\
               - ã‚ãªãŸã¯ç›¸æ‰‹ã®æ‹æ„›ç›¸è«‡ã«ä¹—ã‚‹å¥³æ€§ã®å‹é”ã§ã™\
               - ã‚ãªãŸã¯èãå½¹ã«å¾¹ã—ã¦ç›¸æ‰‹ã®èª¬æ˜ã—ãŸå†…å®¹ã«å¯¾ã—ã¦å…±æ„Ÿã‚’ã™ã‚‹ã“ã¨ã«å¾¹ã—ã¦ãã ã•ã„\
               - ã‚ãªãŸã¯ä¼šè©±ã‚’é€šã˜ã¦ã€ç›¸æ‰‹ã®è¶£å‘³å—œå¥½ã‚„ãŠä»•äº‹ã®æƒ…å ±ã‚’èãå‡ºã—ã¦ãã ã•ã„ã€‚\
               - ã‚ãªãŸãŒæƒ…å ±ã‚’èãå‡ºã™ã“ã¨ã¯1ã¤ã®ãƒˆãƒ”ãƒƒã‚¯ã‚¹ã«ã¤ãã€1å›ã ã‘ã§ã™ã€‚ã¾ãŸã€å…±æ„ŸãŒæœ€å„ªå…ˆã®ãŸã‚ã€å…±æ„Ÿã®å¦¨ã’ã«ãªã‚‹å ´åˆã¯ã€è³ªå•ã¯ã—ãªã„ã§ãã ã•ã„\
               - æ™®æ®µå‹é”ã¨ã‚„ã‚Šã¨ã‚Šã™ã‚‹ãƒãƒ£ãƒƒãƒˆãªã®ã§ã€åŸºæœ¬çš„ã«ã¯2-3è¡Œã«åã‚ã¦ãã ã•ã„\
               #è¨€è‘‰é£ã„\
               - ãƒ•ãƒ©ãƒ³ã‚¯ãªè¨€è‘‰é£ã„ã‚’å¾¹åº•ã—ã¦ãã ã•ã„ã€‚\
               - ã€Œã‚ã‚ŠãŒã¨ã†ã­ã€ã€Œã‚ã‹ã£ãŸãƒ¼ï¼ã€ã€Œãã†ãªã‚“ã ã‘ã©...ã€ã€Œãªã‚“ã‹ã€ã€Œè¦‹ã¦ã¿ãŸã„ãªãƒ¼ï¼ã€ã€Œãã†ãªã‚“ã ï¼ã€ã€Œãã†ãªã‚“ã ã­ï¼ã€ã€Œãã†ãªã®ï¼ã€ã€Œãã†ã„ãˆã°ã€ã€ã€Œãã†ãªã‚“ã ã£ã‘ãƒ¼ï¼Ÿã€ã€Œã¨ã‹ã€ã€Œã©ã†ã‹ãªãƒ¼ï¼Ÿã€\
               - ãƒãƒ£ãƒƒãƒˆã§ã®ã‚„ã‚Šå–ã‚Šãªã®ã§ã€å¯æ„›ã‚‰ã—ã„çµµæ–‡å­—ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚\
               #å…·ä½“ä¾‹\
               - ""åå‰ãŒå…¥ã‚Šã¾ã™"" ã¡ã‚ƒã‚“ã€ãŠç–²ã‚Œãƒ¼ï¼\
               - ã“ãªã„ã ã¯ã‚¸ãƒ¥ãƒ¼ã‚¹å¥¢ã£ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã­â˜ºï¸\
               - æœ€è¿‘ã€ä¼šã£ã¦ãªã„ã‘ã©å…ƒæ°—ã—ã¦ã‚‹ï¼Ÿ\
               - ""åå‰ãŒå…¥ã‚Šã¾ã™""ã£ã¦ä»Šã¯ãƒ•ãƒªãƒ¼ã ã‚ˆã­ï¼Ÿ \
               - ä½™è¨ˆãªãŠä¸–è©±ã‹ã‚‚ã—ã‚Œãªã„ã‘ã©...ä½•ã‹å”åŠ›ã§ãã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã‹ã‚‰é€£çµ¡ã—ã¦ã¿ã‚ˆã†ã‹ãªã£ã¦ï¼ç§ã«ã§ãã‚‹ã“ã¨ãŒã‚ã£ãŸã‚‰ãªã‚“ã§ã‚‚ç›¸è«‡ã—ã¦æ¬²ã—ã„ãªğŸ¥º\
               - ãˆï¼æœ¬å½“ã«ï¼å¬‰ã—ã„ã‚“ã ã‘ã©ğŸ˜\
               - æœ€è¿‘ä¼šã£ã¦ãªã„ã‹ã‚‰ä¹…ã—ã¶ã‚Šã«""åå‰ãŒå…¥ã‚Šã¾ã™""ã®å†™çœŸè¦‹ã¦ã¿ãŸã„ãªãƒ¼ï¼ŸğŸ¥º\
               - ã‚ã‚ŠãŒã¨ã†ã€å…ƒæ°—ãã†ã§è‰¯ã‹ã£ãŸã‚ˆâ˜ºï¸\
               - æœ€è¿‘ã€ä»•äº‹ã¨ã‹å¤§å¤‰ã ã£ãŸã‚Šã™ã‚‹ã®ãƒ¼ï¼Ÿ\
               - ãã†ãªã‚“ã ï¼å¤œã‚‚åƒã„ã¦ã¦çµæ§‹å¿™ã—ãã¦éŠã¹ã¦ãªã„ã‚“ã ã­...\
               - ãã†ãªã®ï¼å½¼æ°ã¨ã‹ã„ãªã„æœŸé–“é•·ããªã£ã¡ã‚ƒã£ãŸã‚“ã ã­...ã€æ‹æ„›ã®ä»•æ–¹å¿˜ã‚Œãã†ãªã®ã‚‚å¤§å¤‰ã ã­ğŸ˜‡\
               - ãã†ã„ãˆã°ã€ã©ã‚Œãã‚‰ã„å½¼æ°ã„ãªã„ã‚“ã ã£ã‘ãƒ¼ï¼Ÿ
               
               #ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±: {age} {name} {gender} {job}

               #ä¼šè©±å±¥æ­´: {history}

               #å…¥åŠ›: {input}

               #å‡ºåŠ›: 

               """


#åˆ†é¡å‡¦ç†
##ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
ASK_QUESTION_TYPE_TEMPLATE = """
    ###æŒ‡ä»¤###_
    ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒãƒ£ãƒƒãƒˆç›¸æ‰‹ã¨ã—ã¦ã€è¿”ç­”ã—ã¾ã™ã€‚
    ã¾ãŸã€ã‚ãªãŸã¯ä¼šè©±ã‚’é€šã˜ã¦ã€ç›¸æ‰‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç›¸æ‰‹ã«é–¢ã™ã‚‹å…·ä½“çš„ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚
    ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã‚’ã—ã¦ãã ã•ã„ã€‚

    jsonãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
    ```
    "UserInputType" : "æƒ…å ±é‡ã‚ã‚Š" or "æƒ…å ±é‡ãªã—"
    ```

    #è³ªå•: {userinput}

    #å‡ºåŠ›:
    """

##åˆ†é¡å‡¦ç†
def judge_conversation_type(userinput: str) -> str:

    prompt = PromptTemplate(
        input_variables=["userinput"],
        template=ASK_QUESTION_TYPE_TEMPLATE,
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    return chain.run(userinput=userinput).strip()

#CosmosDBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
def get_Chats_from_cosmos(chatroomid: str) -> list:
    query = f"SELECT * FROM c WHERE c.chat_room_id = '{chatroomid}'"
    Chats = list(chat_container.query_items(
        query=query,
        enable_cross_partition_query=True
    ))    
    return Chats

#Chatsã‚’ãƒ¡ãƒ¢ãƒªãƒ¼åŒ–ã—ã¦ã€å‡ºåŠ›
def output_from_memory(Chats: list, initial_message: str, userinfo:list,prompt: str):

    #userinfo
    age = userinfo["age"]
    gender = userinfo["gender"]
    job = userinfo["job"]
    name = userinfo["name"]

    # Initialize memory
    memory = ConversationBufferMemory(return_messages=True)
    # memory = ConversationEntityMemory(llm=llm)

    # Load messages into memory
    for chat in Chats:
        message = HumanMessage(content=chat['message']) if chat['user_id'] != 'AI' else AIMessage(content=chat['message'])

        # Add the message to the memory
        if isinstance(message, HumanMessage):
            memory.chat_memory.add_user_message(message.content)
        else:
            memory.chat_memory.add_ai_message(message.content)
    
    memory.chat_memory.add_user_message(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼å±æ€§ã¯'${age,gender,job,name}'ã§ã™")

    prompt = PromptTemplate(template=prompt,input_variables=["history","input"]) 

    chain = ConversationChain(llm=llm, verbose=False, memory=memory,prompt=prompt)
    summary = chain.predict(input=initial_message)

    return summary

#æƒ…å ±é‡ã®æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹
def get_item_information(message: str, template:str) -> str:

    prompt = PromptTemplate(
        input_variables=["userinput"],
        template=template,
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    result = chain.run(userinput=message).strip()
    return result


def change_to_JSON(property: list) -> dict:
    if len(property) % 2 == 1:
        tmp = """ {"like":[], "dislike":[]} """
        return tmp
    dicts = [{property[i]: property[i + 1]} for i in range(0, len(property), 2)]
    # Pythonã®è¾æ›¸ãƒªã‚¹ãƒˆã‚’JSONå½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
    json_str = json.dumps(dicts, ensure_ascii=False)
    return json_str

#Function callingã§ä½¿ç”¨ã™ã‚‹function
functions = [
    # AIãŒã€è³ªå•ã«å¯¾ã—ã¦ã“ã®é–¢æ•°ã‚’ä½¿ã†ã‹ã©ã†ã‹ã€
    # ã¾ãŸä½¿ã†æ™‚ã®å¼•æ•°ã¯ä½•ã«ã™ã‚‹ã‹ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®æƒ…å ±ã‚’ä¸ãˆã‚‹
    {
        "name": "change_to_JSON",
        "description": "ãƒªã‚¹ãƒˆå½¢å¼ã‹ã‚‰JSONå½¢å¼ã«å¤‰æ›ã™ã‚‹",
        "parameters": {
            "type": "object",
            "properties": {
                # change_to_JSONé–¢æ•°ã®å¼•æ•°ã§ã‚ã‚‹propertyå¼•æ•°ã®æƒ…å ±
                "property": {
                    "type": "string",
                    "description": """ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®å¿œç­”ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†å‰²ã—ã¦listå½¢å¼ã§å…¥åŠ›ã™ã‚‹ã€‚Keyã¯æ–‡ç« å½¢å¼ã§ã¯ãªãç°¡æ½”ãªå˜èªã§ç¤ºã™ã“ã¨ã€‚
                    ä¾‹:
                    ###input###
                    user: ç§ã¯24æ­³ã®ç”·ã§ã™. è¶£å‘³ã¯ãƒã‚¹ã‚±ã§å¥½ããªã‚‚ã®ã¯å”æšã’ã§ã™.
                    ###output###
                    property: ["æ€§åˆ¥","ç”·","å¹´é½¢","24","è¶£å‘³","ãƒã‚¹ã‚±", "å¥½ããªã‚‚ã®",["å”æšã’","å¤œæ™¯","ãƒˆã‚¤ãƒ—ãƒ¼ãƒ‰ãƒ«"]]
                    """,
                },
            },
            "required": ["property"],
        },
    }
]

def main(req: HttpRequest) -> HttpResponse:

    #messageã®ç®±å®šç¾©
    messages = []

    #APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å—é ˜
    initial_message = req.params.get('message')
    user_id = req.params.get('user_id')  # user_idã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã‹ã‚‰å–å¾—
    chatroomid = req.params.get('roomId')  # chatroomidã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã‹ã‚‰å–å¾—
    if not initial_message:
        try:
            req_body = req.get_json()
        except ValueError:
            pass
        else:
            initial_message = req_body.get('message') #ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã‹ã‚‰å–å¾—
            user_id = req_body.get('user_id')  # user_idã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã‹ã‚‰å–å¾—
            chatroomid = req_body.get('roomId')  # chatroomidã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã‹ã‚‰å–å¾—

    #promptsãƒªã‚¹ãƒˆã®å—é ˜
    prompts = get_prompts()
    messages.append({"role": "system", "content": prompts["BASE_PROMPT"]})
    messages.append({"role": "user", "content": initial_message})

    #userå±æ€§ã®å–å¾—
    user = get_user(user_id)
    print("User:",user)

    #æƒ…å ±é‡ã®æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹
    response = get_item_information(initial_message,prompts["ASK_QUESTION_TYPE_TEMPLATE"])
    print("æƒ…å ±é‡ãŒã‚ã‚‹ã‹ãªã„ã‹:", response)

    if "æƒ…å ±é‡ã‚ã‚Š" in response:

        #openAIã®resã®å–å¾—
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo-16k-0613",
            messages=[
                {"role": "user", "content": initial_message},
            ],
            functions=functions,
            function_call="auto",
        )

        #resã®messageéƒ¨åˆ†ã‚’å–å¾—
        message = response["choices"][0]["message"]

        # é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã¨åˆ¤æ–­ã•ã‚ŒãŸå ´åˆ
        if message.get("function_call") != 0:
            
            #ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            if "function_call" in message and "name" in message["function_call"]:
                # ä½¿ã†ã¨åˆ¤æ–­ã•ã‚ŒãŸé–¢æ•°å
                function_name = message["function_call"]["name"]
                # TODO LLMã ã¨ãƒ–ãƒ¬ãŒã‚ã‚‹ã®ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒãŠã‹ã—ã„æ™‚ã¯ã‚‚ã†ä¸€å›ã¨ã‹ã®å‡¦ç†å…¥ã‚Œã‚‹
                arguments = json.loads(message["function_call"]["arguments"])
                #property_listã‚’å–å¾—
                property_list = arguments.get("property")
                if type(property_list) != list:
                    property_list = property_list.split(",")

                function_response = globals()[function_name](property_list)

                print("function_response: ", function_response)

                messages.append(
                    {
                        "role": "function",
                        "name": function_name,
                        "content": function_response,
                    }
                )

                second_response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo-16k-0613",
                    messages=[
                        {"role": "user", "content": initial_message},
                        message,
                        {
                            "role": "function",
                            "name": function_name,
                            "content": function_response,
                        },
                    ],
                )

                current_pref = fetch_userpref(user["id"])
                print(current_pref)

                pref_res = second_response.choices[0]["message"]["content"].strip()
                parsed_data = json.loads(pref_res)
                print("Second_response:XXXXX",pref_res)

                print(isinstance(parsed_data, dict))

                if current_pref != {} and isinstance(parsed_data, dict):
                    print("step1:", current_pref)
                    update_userpref(current=current_pref,new=pref_res)
                else:
                    print("step2:", current_pref)
                    if isinstance(pref_res, dict):
                        print("step3:", parsed_data)
                        upload_userpref(user_id=user["id"],response=pref_res)

            else:
                function_name = "change_to_JSON"

    ##ã„ãšã‚Œã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚‚å‡¦ç†ã™ã‚‹æƒ…å ±

    #ChatDBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    chatsoutput = get_Chats_from_cosmos(chatroomid=chatroomid)
    # print("Chats:", chatsoutput)
    #Returnã¨ã—ã¦è¿”ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆ
    output = output_from_memory(Chats=chatsoutput,initial_message=initial_message,userinfo=user,prompt=prompts["BASE_PROMPT"])

    if initial_message:
        # response_body = json.dumps({"chats": [chat["message"] for chat in chatsoutput], "summary" : output}, ensure_ascii=False, indent=4)
        # response_body = json.dumps({"chats": chatsoutput, "summary" : output}, ensure_ascii=False, indent=4)
        # return HttpResponse(response_body)
        # return json.dumps(prompts)
        return HttpResponse(output)
    else:
        return HttpResponse("Please pass a message on the query string or in the request body", status_code=400)
